{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a0fa203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      company     code\n",
      "0        삼성전자  '005930\n",
      "1      SK하이닉스  '000660\n",
      "2    LG에너지솔루션  '373220\n",
      "3    삼성바이오로직스  '207940\n",
      "4         현대차  '005380\n",
      "..        ...      ...\n",
      "195   LX인터내셔널  '001120\n",
      "196     SK케미칼  '285130\n",
      "197      롯데렌탈  '089860\n",
      "198      일진전기  '103590\n",
      "199     코오롱인더  '120110\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.parse\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "url = 'https://finance.naver.com/sise/sise_market_sum.nhn?&page='\n",
    "\n",
    "company_data = []\n",
    "count = 0\n",
    "\n",
    "for page in range(1, 40):\n",
    "   res = requests.get(url + str(page))\n",
    "   html = res.content.decode('euc-kr', 'replace')\n",
    "   soup = BeautifulSoup(html, 'lxml')\n",
    "   stock_table = soup.find('table', attrs={'class': 'type_2'})\n",
    "   stock_rows = stock_table.find_all('tr', onmouseover=True)\n",
    "\n",
    "   for row in stock_rows:\n",
    "       if count >= 200:\n",
    "           break\n",
    "\n",
    "       stock_data = row.find_all('td')\n",
    "\n",
    "       if len(stock_data) > 1:\n",
    "           per = stock_data[11].text.strip()\n",
    "           if per != 'N/A':\n",
    "               company_name = stock_data[1].text.strip()\n",
    "               company_code = stock_data[1].find('a')['href'].split('code=')[-1]\n",
    "               company_code = \"'\" + company_code.zfill(6)\n",
    "\n",
    "               company_data.append([company_name, company_code, per])\n",
    "               count += 1\n",
    "\n",
    "df = pd.DataFrame(data=company_data, columns=['company', 'code', 'PER'])\n",
    "df = df.drop('PER', axis=1)\n",
    "df.to_csv('kopsi_200_stocks.csv', index=False, encoding='utf-8-sig')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b39213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import urllib.parse\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def collect_news_for_date(search_query, code, date):\n",
    "    news_list = []\n",
    "    encoded_query = urllib.parse.quote(search_query.encode('euc-kr'))\n",
    "    url = f\"https://finance.naver.com/news/news_search.naver?q={encoded_query}&sm=all.basic&pd=1&stDateStart={date}&stDateEnd={date}\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'euc-kr'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('dd', class_='articleSubject')[:1]  # Limit to 1 news title per date\n",
    "    for article in articles:\n",
    "        title = article.find('a').text.strip()\n",
    "        news_list.append({'company': search_query, 'code': code, 'Date': date, 'Title': title})\n",
    "    return news_list\n",
    "\n",
    "def collect_and_save_all_news(df, start_date, end_date, max_workers=10):\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    all_news = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for _, row in df.iterrows():\n",
    "            company = row['company']\n",
    "            code = row['code']\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                futures.append(executor.submit(collect_news_for_date, company, code, current_date.strftime('%Y-%m-%d')))\n",
    "                current_date += timedelta(days=7)  # 7일 간격으로 검색\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            news = future.result()\n",
    "            if news:\n",
    "                all_news.extend(news)\n",
    "\n",
    "    df = pd.DataFrame(all_news)\n",
    "    df.to_csv('all_news.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "df = pd.read_csv('kopsi_200_stocks.csv')\n",
    "collect_and_save_all_news(df, '2021-01-01', '2023-12-31', max_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f2ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
